{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Icecream0507/KhailGen/blob/main/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# 音频扩散模型训练笔记本 (从 Hugging Face 加载数据)\n",
        "\n",
        "这是一个用于训练音频扩散模型的 Google Colab 笔记本。它会自动从 Hugging Face Hub 加载 `Ice144/KhailGen` 数据集。\n",
        "\n",
        "**使用说明：**\n",
        "1.  从上到下依次运行每个单元格。\n",
        "2.  在\"挂载 Google Drive\"单元格中，根据提示进行授权，以便保存处理后的数据和模型。\n",
        "3.  运行第4节中的代码，它会自动下载并处理数据集，然后保存为 `.npy` 文件，以供训练使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## 1. 环境准备和库安装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install-deps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aceaadc-7ca3-44e3-b529-725ca5251a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.12/dist-packages (12.0.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.12/dist-packages (2.6.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from pynvml) (12.575.51)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.12/dist-packages (from tensorboardX) (5.29.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.14.1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install numpy tqdm PyYAML soundfile pynvml tensorboardX\n",
        "# Install Hugging Face datasets and librosa for audio processing\n",
        "!pip install datasets librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-gdrive"
      },
      "source": [
        "## 2. 挂载 Google Drive\n",
        "\n",
        "这一步是为了持久化存储数据和模型，避免每次运行都重新下载和处理。你需要点击链接并授权。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mount-code",
        "outputId": "1c2a31e2-0620-4e0e-bc18-8950e405dad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "create-config"
      },
      "source": [
        "## 3. 创建配置文件 `config.yaml`\n",
        "\n",
        "训练和数据处理所需的配置参数。请确保 `processed_folder` 和 `model_folder` 路径正确。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "write-config",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f54e6f3-314e-4184-8f77-07f2332539d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.yaml\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.yaml\n",
        "# Training configuration\n",
        "epochs: 1000\n",
        "learning_rate: 0.0002\n",
        "batch_size: 1\n",
        "timesteps: 200\n",
        "\n",
        "# Dataset configuration\n",
        "# The folder where processed .npy files will be saved\n",
        "processed_folder: /content/drive/MyDrive/khailgen_processed_data\n",
        "audio_folder: /content/drive/MyDrive/KhailGen/data/wav\n",
        "waveform_length: 1323000 # 1 second at 44.1kHz\n",
        "sample_rate: 22050\n",
        "channels: 1 # 1 for mono, 2 for stereo\n",
        "\n",
        "# Model configuration\n",
        "embedding_dim: 128\n",
        "\n",
        "# WaveNet specific parameters\n",
        "res_channels: 64\n",
        "skip_channels: 64\n",
        "num_res_layers: 8\n",
        "dilation_cycle: 4\n",
        "\n",
        "# Saving configuration\n",
        "model_folder: /content/drive/MyDrive/khailgen_models\n",
        "model_path: /content/drive/MyDrive/khailgen_models/wave_diffusion_best_model.pth\n",
        "log_dir: /content/drive/MyDrive/khailgen_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess-section"
      },
      "source": [
        "## 4. 从 Hugging Face 加载并预处理数据集\n",
        "\n",
        "这个单元格会从 Hugging Face Hub 下载 `Ice144/KhailGen` 数据集，然后对音频进行重采样、归一化和分割，最后保存为 `.npy` 文件。这个过程可能需要一些时间。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets librosa soundfile tqdm torchcodec torchaudio\n"
      ],
      "metadata": {
        "id": "s4HbIRB4CdrG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SeeK1-HzMRuR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SPfjNOQTMTW",
        "outputId": "4c926539-d838-4f9a-be5c-826b86ec566e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Fri Aug 22 11:18:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "load-preprocess-hf"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import librosa\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import yaml\n",
        "# from datasets import load_dataset\n",
        "# import torch\n",
        "# import torchcodec\n",
        "# import soundfile as sf\n",
        "# from IPython.display import Audio as play_audio\n",
        "\n",
        "\n",
        "# import torchaudio\n",
        "# try:\n",
        "#     torchaudio.set_audio_backend(\"soundfile\")\n",
        "#     print(\"使用SoundFile后端\")\n",
        "# except:\n",
        "#     print(\"SoundFile不可用，使用默认后端\")\n",
        "\n",
        "\n",
        "# print(torch.cuda.is_available())\n",
        "\n",
        "\n",
        "# config_path = 'config.yaml'\n",
        "\n",
        "# def load_config(config_path):\n",
        "#     with open(config_path, 'r') as f:\n",
        "#         config = yaml.safe_load(f)\n",
        "#     return config\n",
        "\n",
        "# config = load_config(config_path)\n",
        "\n",
        "# # def process_and_save_dataset(dataset_name, processed_folder, sr, fixed_length):\n",
        "# #     \"\"\"\n",
        "# #     Loads an audio dataset from Hugging Face, processes it, and saves it as .npy files.\n",
        "# #     \"\"\"\n",
        "# #     os.makedirs(processed_folder, exist_ok=True)\n",
        "\n",
        "# #     # Load the dataset from Hugging Face\n",
        "# #     print(f\"Loading dataset '{dataset_name}' from Hugging Face...\")\n",
        "# #     dataset = load_dataset(dataset_name)\n",
        "\n",
        "# #     # Access the 'train' split and the 'audio' column\n",
        "# #     audio_data = dataset['train']['audio']\n",
        "\n",
        "\n",
        "# #         # 调试：检查第一个样本的详细信息\n",
        "# #     first_sample = dataset['train'][0]\n",
        "# #     first_audio = first_sample['audio'].get_samples_played_in_range(0, 10)\n",
        "\n",
        "\n",
        "\n",
        "# #     play_audio(first_audio.data.cpu().numpy(), rate=first_audio.sample_rate)\n",
        "\n",
        "\n",
        "# #     print(type(first_audio.metadata))\n",
        "\n",
        "\n",
        "# #     all_samples = first_audio.get_all_samples().data\n",
        "\n",
        "# #     print(all_samples)\n",
        "\n",
        "# #     print(f\"Starting pre-processing of {len(audio_data)} audio samples...\")\n",
        "# #     with tqdm(total=len(audio_data), desc=\"Processing audio samples\") as pbar:\n",
        "# #         for i, sample in enumerate(audio_data):\n",
        "# #           try:\n",
        "# #               y, sr = librosa.load(sample['array'], sr=sr, mono = True)\n",
        "\n",
        "# #               # Normalize to [-1, 1]\n",
        "# #               y = y / np.max(np.abs(y))\n",
        "\n",
        "# #               # Split into fixed-length segments and save\n",
        "# #               waveform_len = y.shape[0]\n",
        "# #               num_segments = int(np.ceil(waveform_len / fixed_length))\n",
        "\n",
        "# #               for j in range(num_segments):\n",
        "# #                   start_idx = j * fixed_length\n",
        "# #                   end_idx = start_idx + fixed_length\n",
        "\n",
        "# #                   if end_idx <= waveform_len:\n",
        "# #                       segment = y[start_idx:end_idx]\n",
        "# #                   else:\n",
        "# #                       segment = y[start_idx:]\n",
        "# #                       pad_width = fixed_length - len(segment)\n",
        "# #                       segment = np.pad(segment, (0, pad_width), mode='constant')\n",
        "\n",
        "# #                   segment_file_name = f\"audio_sample_{i:04d}_segment_{j:04d}.npy\"\n",
        "# #                   processed_file_path = os.path.join(processed_folder, segment_file_name)\n",
        "\n",
        "# #                   np.save(processed_file_path, segment)\n",
        "\n",
        "# #           except Exception as e:\n",
        "# #               print(f\"Error processing sample {i}: {e}\")\n",
        "# #               continue\n",
        "\n",
        "# #           pbar.update(1)\n",
        "\n",
        "# #     print(\"Audio pre-processing complete.\")\n",
        "\n",
        "# def process_and_save_audio(audio_folder, processed_folder, sr, fixed_length):\n",
        "#     \"\"\"\n",
        "#     加载、处理原始音频文件，将其分割为固定长度的片段，并保存为 .npy 格式。\n",
        "#     \"\"\"\n",
        "#     os.makedirs(processed_folder, exist_ok=True)\n",
        "\n",
        "#     audio_files = [os.path.join(audio_folder, f) for f in os.listdir(audio_folder)\n",
        "#                   if f.endswith('.wav') or f.endswith('.mp3')]\n",
        "\n",
        "#     print(f\"Starting pre-processing of {len(audio_files)} audio files...\")\n",
        "#     with tqdm(total=len(audio_files), desc=\"Processing audio files\") as pbar:\n",
        "#         for file_path in audio_files:\n",
        "#             file_name = os.path.basename(file_path)\n",
        "#             base_name = os.path.splitext(file_name)[0]\n",
        "\n",
        "#             try:\n",
        "#                 # 保持 mono=True，正确加载为单声道\n",
        "#                 y, _ = librosa.load(file_path, sr=sr, mono=True)\n",
        "\n",
        "#                 # 归一化到 [-1, 1]\n",
        "#                 y = y / np.max(np.abs(y))\n",
        "\n",
        "#                 # 计算需要分割的片段数量\n",
        "#                 waveform_len = y.shape[0]\n",
        "#                 num_segments = int(np.ceil(waveform_len / fixed_length))\n",
        "\n",
        "#                 # 处理每个片段\n",
        "#                 for i in range(num_segments):\n",
        "#                     start_idx = i * fixed_length\n",
        "#                     end_idx = start_idx + fixed_length\n",
        "\n",
        "#                     # 提取片段\n",
        "#                     if end_idx <= waveform_len:\n",
        "#                         segment = y[start_idx:end_idx]\n",
        "#                     else:\n",
        "#                         # 最后一个片段，需要填充\n",
        "#                         segment = y[start_idx:]\n",
        "#                         pad_width = fixed_length - len(segment)\n",
        "#                         segment = np.pad(segment, (0, pad_width), mode='constant')\n",
        "\n",
        "#                     # 生成唯一的文件名\n",
        "#                     segment_file_name = f\"{base_name}_segment_{i:04d}.npy\"\n",
        "#                     processed_file_path = os.path.join(processed_folder, segment_file_name)\n",
        "#                     # 保存片段\n",
        "#                     np.save(processed_file_path, segment)\n",
        "\n",
        "#             except Exception as e:\n",
        "#                 print(f\"Error processing {file_name}: {e}\")\n",
        "#                 continue\n",
        "\n",
        "#             pbar.update(1)\n",
        "\n",
        "#     print(\"Audio pre-processing complete.\")\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     process_and_save_audio(\n",
        "#         audio_folder=config[\"audio_folder\"],\n",
        "#         processed_folder=config[\"processed_folder\"],\n",
        "#         sr=config[\"sample_rate\"],\n",
        "#         fixed_length=config[\"waveform_length\"]\n",
        "#     )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-code-block"
      },
      "source": [
        "## 5. 整合所有模型和训练代码\n",
        "\n",
        "这里包含了所有模型、数据处理和训练逻辑的代码，它们将直接在内存中运行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "combined-code"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import yaml\n",
        "import soundfile as sf\n",
        "import pynvml\n",
        "import math\n",
        "\n",
        "# ==================== Data Utilities ====================\n",
        "def pad_to_fixed_length_waveform(waveform, max_len):\n",
        "    \"\"\"\n",
        "    Pads or truncates a 1D waveform to a fixed length.\n",
        "    \"\"\"\n",
        "    waveform_len = waveform.shape[0]\n",
        "    if waveform_len < max_len:\n",
        "        pad_width = max_len - waveform_len\n",
        "        waveform = np.pad(waveform, (0, pad_width), mode='constant')\n",
        "    elif waveform_len > max_len:\n",
        "        waveform = waveform[:max_len]\n",
        "    return waveform\n",
        "\n",
        "class AudioDatasetWaveform(Dataset):\n",
        "    \"\"\"\n",
        "    Custom audio dataset to load waveform data from pre-processed .npy files.\n",
        "    \"\"\"\n",
        "    def __init__(self, processed_folder, fixed_length):\n",
        "        self.processed_files = [\n",
        "            os.path.join(processed_folder, f)\n",
        "            for f in os.listdir(processed_folder)\n",
        "            if f.endswith('.npy')\n",
        "        ]\n",
        "        self.fixed_length = fixed_length\n",
        "        print(f\"Found {len(self.processed_files)} pre-processed audio files...\")\n",
        "\n",
        "        print(\"Loading all waveforms into memory...\")\n",
        "        self.waveforms = []\n",
        "        with tqdm(total=len(self.processed_files), desc=\"Loading .npy files\") as pbar:\n",
        "            for file_path in self.processed_files:\n",
        "                try:\n",
        "                    waveform = np.load(file_path)\n",
        "                    if waveform.ndim == 2 and waveform.shape[0] == 1:\n",
        "                        waveform = waveform.squeeze(0)\n",
        "                    elif waveform.ndim != 1:\n",
        "                        print(f\"Warning: Skipping {file_path} due to unexpected dimensions: {waveform.shape}\")\n",
        "                        continue\n",
        "                    waveform = pad_to_fixed_length_waveform(waveform, self.fixed_length)\n",
        "                    self.waveforms.append(waveform)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading {file_path}: {e}\")\n",
        "                pbar.update(1)\n",
        "\n",
        "        if not all(w.shape[0] == self.fixed_length for w in self.waveforms):\n",
        "            print(\"Warning: Waveform lengths are not consistent. Some files might be corrupted.\")\n",
        "\n",
        "        print(\"All waveforms loaded.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.waveforms)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform = self.waveforms[idx]\n",
        "        return torch.tensor(waveform, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "# ==================== WaveNet Model ====================\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):\n",
        "        super(Conv, self).__init__()\n",
        "        self.padding = dilation * (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, dilation=dilation, padding=self.padding)\n",
        "        self.conv = nn.utils.weight_norm(self.conv)\n",
        "        nn.init.kaiming_normal_(self.conv.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ZeroConv1d(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(ZeroConv1d, self).__init__()\n",
        "        self.conv = nn.Conv1d(in_channel, out_channel, kernel_size=1, padding=0)\n",
        "        self.conv.weight.data.zero_()\n",
        "        self.conv.bias.data.zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, res_channels, skip_channels, dilation, time_embed_dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.res_channels = res_channels\n",
        "        self.fc_t = nn.Linear(time_embed_dim, res_channels)\n",
        "        self.dilated_conv = Conv(res_channels, 2 * res_channels, kernel_size=3, dilation=dilation)\n",
        "        self.res_conv = Conv(res_channels, res_channels, kernel_size=1)\n",
        "        self.skip_conv = Conv(res_channels, skip_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x, time_embed = input_data\n",
        "        h = x\n",
        "        time_part = self.fc_t(time_embed).unsqueeze(-1)\n",
        "        h = h + time_part\n",
        "        h = self.dilated_conv(h)\n",
        "        out = torch.tanh(h[:, :self.res_channels, :]) * torch.sigmoid(h[:, self.res_channels:, :])\n",
        "        res = self.res_conv(out)\n",
        "        skip = self.skip_conv(out)\n",
        "        return (x + res, skip)\n",
        "\n",
        "class ResidualGroup(nn.Module):\n",
        "    def __init__(self, res_channels, skip_channels, num_res_layers, dilation_cycle, time_embed_dim):\n",
        "        super(ResidualGroup, self).__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            ResidualBlock(\n",
        "                res_channels=res_channels,\n",
        "                skip_channels=skip_channels,\n",
        "                dilation=2**(i % dilation_cycle),\n",
        "                time_embed_dim=time_embed_dim\n",
        "            )\n",
        "            for i in range(num_res_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x, time_embed):\n",
        "        skip_connections = 0\n",
        "        for layer in self.layers:\n",
        "            x, skip = layer((x, time_embed))\n",
        "            skip_connections = skip_connections + skip\n",
        "        return skip_connections\n",
        "\n",
        "class WaveNet1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, time_embedding_dim, res_channels, skip_channels, num_res_layers, dilation_cycle):\n",
        "        super(WaveNet1D, self).__init__()\n",
        "        self.time_embedding_dim = time_embedding_dim\n",
        "\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            nn.Linear(time_embedding_dim, time_embedding_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(time_embedding_dim * 4, res_channels)\n",
        "        )\n",
        "\n",
        "        self.init_conv = nn.Sequential(\n",
        "            Conv(in_channels, res_channels, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.residual_group = ResidualGroup(\n",
        "            res_channels=res_channels,\n",
        "            skip_channels=skip_channels,\n",
        "            num_res_layers=num_res_layers,\n",
        "            dilation_cycle=dilation_cycle,\n",
        "            time_embed_dim=res_channels\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Sequential(\n",
        "            Conv(skip_channels, skip_channels, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            ZeroConv1d(skip_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        time_embed = self.sinusoidal_embedding(t, self.time_embedding_dim)\n",
        "        time_embed = self.time_mlp(time_embed)\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        x = self.residual_group(x, time_embed)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "    def sinusoidal_embedding(self, t, dim):\n",
        "        half_dim = dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=t.device) * -embeddings)\n",
        "        embeddings = t.float()[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n",
        "# ==================== Diffusion Model ====================\n",
        "\n",
        "class DiffusionModel(torch.nn.Module):\n",
        "    def __init__(self, unet, timesteps=1000, device='cpu'):\n",
        "        super(DiffusionModel, self).__init__()\n",
        "        self.unet = unet\n",
        "        self.timesteps = timesteps\n",
        "        self.device = device\n",
        "\n",
        "        betas = torch.linspace(1e-4, 0.02, timesteps).to(device)\n",
        "        alphas = 1. - betas\n",
        "        alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas', alphas)\n",
        "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
        "\n",
        "    def forward_diffusion(self, x_start, t, noise=None):\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)\n",
        "\n",
        "        sqrt_alphas_cumprod_t = torch.sqrt(self.alphas_cumprod[t])[:, None, None]\n",
        "        sqrt_one_minus_alphas_cumprod_t = torch.sqrt(1. - self.alphas_cumprod[t])[:, None, None]\n",
        "\n",
        "        x_t = sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    def get_noise_prediction(self, x_t, t):\n",
        "        return self.unet(x_t, t)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, sample_shape):\n",
        "        batch_size = sample_shape[0]\n",
        "        channels = sample_shape[1]\n",
        "        waveform_length = sample_shape[2]\n",
        "        x_t = torch.randn(batch_size, channels, waveform_length, device=self.device)\n",
        "\n",
        "        for t in reversed(range(self.timesteps)):\n",
        "            t_tensor = torch.full((batch_size,), t, device=self.device, dtype=torch.long)\n",
        "            predicted_noise = self.get_noise_prediction(x_t, t_tensor)\n",
        "            alpha_t = self.alphas[t]\n",
        "            alpha_t_cumprod = self.alphas_cumprod[t]\n",
        "\n",
        "            mean = 1.0 / torch.sqrt(alpha_t) * (x_t - (1.0 - alpha_t) / torch.sqrt(1.0 - alpha_t_cumprod) * predicted_noise)\n",
        "            variance = self.betas[t]\n",
        "\n",
        "            if t > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "                x_t = mean + torch.sqrt(variance) * noise\n",
        "            else:\n",
        "                x_t = mean\n",
        "\n",
        "        return x_t\n",
        "\n",
        "# ==================== Training Utilities ====================\n",
        "\n",
        "def get_gpu_memory_info():\n",
        "    \"\"\"Gets the GPU memory usage for the first GPU.\"\"\"\n",
        "    try:\n",
        "        pynvml.nvmlInit()\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
        "        info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
        "        free_memory_mib = info.free / (1024 ** 2)\n",
        "        total_memory_mib = info.total / (1024 ** 2)\n",
        "        print(f\"GPU 0 - Total: {total_memory_mib:.2f} MiB, Free: {free_memory_mib:.2f} MiB\")\n",
        "        pynvml.nvmlShutdown()\n",
        "        return free_memory_mib / 1024\n",
        "    except pynvml.NVMLError as error:\n",
        "        print(f\"Error getting GPU memory info: {error}\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def train(config_path):\n",
        "    with open(config_path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "\n",
        "    print(\"Initializing dataset and model...\")\n",
        "    dataset = AudioDatasetWaveform(\n",
        "        processed_folder=config[\"processed_folder\"],\n",
        "        fixed_length=config[\"waveform_length\"]\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    wavenet = WaveNet1D(\n",
        "        in_channels=config[\"channels\"],\n",
        "        out_channels=config[\"channels\"],\n",
        "        time_embedding_dim=config[\"embedding_dim\"],\n",
        "        res_channels=config[\"res_channels\"],\n",
        "        skip_channels=config[\"skip_channels\"],\n",
        "        num_res_layers=config[\"num_res_layers\"],\n",
        "        dilation_cycle=config[\"dilation_cycle\"]\n",
        "    ).to(device)\n",
        "\n",
        "    diffusion_model = DiffusionModel(wavenet, timesteps=config[\"timesteps\"], device=device).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(wavenet.parameters(), lr=config[\"learning_rate\"])\n",
        "    loss_fn = nn.MSELoss()\n",
        "    epochs = config[\"epochs\"]\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    audio_save_dir = \"temp_noisy_audios\"\n",
        "    os.makedirs(audio_save_dir, exist_ok=True)\n",
        "\n",
        "    t_to_save = [50, 100, 200, 300, 400, 500]\n",
        "    saved_t_vals = set()\n",
        "    best_epoch_loss = float('inf')\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_epoch_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, waveform_batch in enumerate(dataloader):\n",
        "            waveform_batch = waveform_batch.to(device)\n",
        "\n",
        "            try:\n",
        "                t = torch.randint(0, diffusion_model.timesteps, (waveform_batch.shape[0],), device=device).long()\n",
        "                x_t, noise = diffusion_model.forward_diffusion(waveform_batch, t)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    predicted_noise = wavenet(x_t, t)\n",
        "                    loss = loss_fn(predicted_noise, noise)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                total_epoch_loss += loss.item()\n",
        "                global_step += 1\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    end_time = time.time()\n",
        "                    duration = end_time - start_time\n",
        "                    print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}, Time: {duration:.2f}s\")\n",
        "                    start_time = time.time()\n",
        "\n",
        "                if (i + 1) % 500 == 0 and len(saved_t_vals) < len(t_to_save):\n",
        "                    for t_val in t_to_save:\n",
        "                        if t_val not in saved_t_vals:\n",
        "                            random_waveform = waveform_batch[0]\n",
        "                            t_tensor = torch.full((1,), t_val, device=device).long()\n",
        "                            noisy_waveform, _ = diffusion_model.forward_diffusion(random_waveform.unsqueeze(0), t_tensor)\n",
        "\n",
        "                            noisy_waveform_np = noisy_waveform.squeeze().cpu().numpy()\n",
        "                            sf.write(os.path.join(audio_save_dir, f'noisy_t{t_val}.wav'), noisy_waveform_np, 44100)\n",
        "                            saved_t_vals.add(t_val)\n",
        "                            print(f\"Saved noisy audio for t={t_val}\")\n",
        "\n",
        "            except RuntimeError as e:\n",
        "                if \"out of memory\" in str(e):\n",
        "                    print(f\"{e}\\nWARNING: Out of memory at step {i+1}. Skipping batch and clearing cache.\")\n",
        "                    torch.cuda.empty_cache()\n",
        "                    time.sleep(5)\n",
        "                    print(\"GPU memory status after clearing cache:\")\n",
        "                    print(torch.cuda.memory_stats())\n",
        "                    del wavenet\n",
        "                    del diffusion_model\n",
        "                    del loss_fn\n",
        "                    del scaler\n",
        "                    return False\n",
        "                else:\n",
        "                    raise e\n",
        "\n",
        "        avg_epoch_loss = total_epoch_loss / len(dataloader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} finished, Average Loss: {avg_epoch_loss:.4f}\")\n",
        "        if avg_epoch_loss < best_epoch_loss:\n",
        "            best_epoch_loss = avg_epoch_loss\n",
        "            model_save_path = os.path.join(config['model_folder'], \"wave_diffusion_best_model.pth\")\n",
        "            torch.save(wavenet.state_dict(), model_save_path)\n",
        "            print(f\"New best model saved! Average Loss: {best_epoch_loss:.4f}\")\n",
        "\n",
        "        model_save_path = os.path.join(config['model_folder'], \"wave_diffusion_current_model.pth\")\n",
        "        torch.save(wavenet.state_dict(), model_save_path)\n",
        "\n",
        "    print(\"\\nTraining finished.\")\n",
        "    time_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    version = 0\n",
        "    while True:\n",
        "        model_save_path = f\"diffusion_{time_stamp}_{config['waveform_length']/config['sample_rate']}_v{version}.pth\"\n",
        "        model_save_path = config['model_folder'] + \"/\" + model_save_path\n",
        "        try:\n",
        "            with open(model_save_path, 'x'):\n",
        "                break\n",
        "        except FileExistsError:\n",
        "            version += 1\n",
        "    torch.save(wavenet.state_dict(), model_save_path)\n",
        "    print(f\"Final model saved at: {model_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "start-training"
      },
      "source": [
        "## 6. 启动训练\n",
        "\n",
        "现在，你可以运行以下代码来启动训练过程。训练时，模型文件和日志将保存到你 Google Drive 指定的路径下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-main",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef2b7f8-187c-4350-a264-c7e1478f8716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing dataset and model...\n",
            "Found 811 pre-processed audio files...\n",
            "Loading all waveforms into memory...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading .npy files: 100%|██████████| 811/811 [00:25<00:00, 31.33it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All waveforms loaded.\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2505529586.py:283: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/tmp/ipython-input-2505529586.py:305: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Step [100/811], Loss: 0.5916, Time: 27.53s\n",
            "Epoch [1/1000], Step [200/811], Loss: 0.0992, Time: 25.74s\n",
            "Epoch [1/1000], Step [300/811], Loss: 0.0651, Time: 25.76s\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config_path = 'config.yaml'\n",
        "    if not os.path.exists(config_path):\n",
        "        raise FileNotFoundError(f\"Config file not found at: {config_path}. Please run the previous cell.\")\n",
        "\n",
        "    # 启动训练\n",
        "    train(config_path)"
      ]
    }
  ]
}